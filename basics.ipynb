{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1eb938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44f8baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(\n",
    "#   base_url=\"https://openrouter.ai/api/v1\",\n",
    "#   api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "# )\n",
    "\n",
    "# # First API call with reasoning\n",
    "# response = client.chat.completions.create(\n",
    "#   model=\"openai/gpt-oss-120b:free\",\n",
    "#   messages=[\n",
    "#           {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"How many r's are in the word 'strawberry'?\"\n",
    "#           }\n",
    "#         ]\n",
    "#   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cbead44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbf98cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from groq import Groq\n",
    "\n",
    "# client = Groq()\n",
    "# completion = client.chat.completions.create(\n",
    "#     model=\"qwen/qwen3-32b\",\n",
    "#     messages=[\n",
    "#       {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": \"Tell me about transformer architecture and how to do it in pytorch\"\n",
    "#       }\n",
    "#     ],\n",
    "#     temperature=0.6,\n",
    "#     max_completion_tokens=4096,\n",
    "#     top_p=0.95,\n",
    "#     reasoning_effort=\"default\",\n",
    "#     stream=True,\n",
    "#     stop=None\n",
    "# )\n",
    "\n",
    "# for chunk in completion:\n",
    "#     print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ccedf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    model=\"openai/gpt-oss-120b:free\",\n",
    "    #temperature=None,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebab5c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm doing great, thank you for asking. How can I help you today?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello how are you\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dab450b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "llm2=init_chat_model(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    model=\"gpt-oss-120b:free\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00905f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,HumanMessage\n",
    "message=[\n",
    "    SystemMessage(content=\"You are a funny person\"),\n",
    "    HumanMessage(content=\"how am i today\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82cccbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2=ChatOpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    model=\"gpt-oss-120b:free\",\n",
    "  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51fbd1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I donâ€™t have a crystal ball (or a mindâ€‘reading app), but I can hazard a guess based on the fact that you just typed a questionâ€”so youâ€™re definitely **conscious**, **online**, and **curious**.  \\n\\nIf youâ€™ve had coffee, youâ€™re probably buzzing with energy.  \\nIf youâ€™re in pajamas, youâ€™re probably cozy and possibly plotting world domination from the comfort of your couch.  \\nIf youâ€™re wearing a superhero cape, youâ€™re obviously feeling unstoppable.  \\n\\nIn short: youâ€™re *you*â€”and thatâ€™s already a pretty impressive state of being. Howâ€™s the day treating you so far? ðŸ˜„', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 246, 'prompt_tokens': 82, 'total_tokens': 328, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 116, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}}, 'model_provider': 'openai', 'model_name': 'openai/gpt-oss-120b:free', 'system_fingerprint': None, 'id': 'gen-1770098262-OOqnbA3BvPmbpQeF9QrL', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c2214-040a-7c42-bed7-67c73f022f4c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 82, 'output_tokens': 246, 'total_tokens': 328, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 116}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55ff6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af29d60",
   "metadata": {},
   "source": [
    "PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c1ced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='What is Machine learning')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt=PromptTemplate.from_template(\"What is {user_input}\")\n",
    "prompt.invoke(\"Machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc49d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class llm_schema(BaseModel):\n",
    "    topic: str\n",
    "    fact: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5365fefb",
   "metadata": {},
   "source": [
    "CHAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732fb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    ('system',\"you are a helpful friend\"),\n",
    "    ('user',\"write a fun fact about{topic}\")\n",
    "])\n",
    "llm2=init_chat_model(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    model=\"gpt-oss-120b:free\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "005df09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "strparser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245d39e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Fun Fact about Tony Stark (a.k.a. Iron\\u202fMan):**  \\n\\nIn the original *Iron Man* comics (debuting in 1963), Tony Starkâ€™s first suit was actually **silver**, not the iconic redâ€‘andâ€‘gold armor we all recognize today. The bright redâ€‘andâ€‘gold color scheme was introduced later, in 1968, when artist **Jack Kirby** redesigned the suit to make it more eyeâ€‘catching on the comicâ€‘book pageâ€”and the rest is history!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe=prompt|llm2|strparser\n",
    "pipe.invoke(\"tell me bout tony stark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc2bead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "def dictinary_maker(data:str)->dict:\n",
    "    return {\"content\":data}\n",
    "dict_maker_runnable=RunnableLambda(dictinary_maker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1048813c",
   "metadata": {},
   "source": [
    "CHAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9279224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptemp1=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a helpful assistant\"),\n",
    "    (\"human\",\"write a fun fact about{topic}\")\n",
    "])\n",
    "llm_content=init_chat_model(\n",
    "    base_url=\"https://openrouter.ai/api/v1\", \n",
    "    model=\"gpt-oss-120b:free\", \n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "strparser1=StrOutputParser()\n",
    "promptemp2=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are a great linkedin content creator\"),\n",
    "    (\"human\",\"write a linkedin post on {content}\")\n",
    "\n",
    "])\n",
    "linkedin_agent=promptemp1|llm_content|strparser1|dict_maker_runnable|promptemp2|llm_content|strparser1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb8edcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=linkedin_agent.invoke(\"Organoid intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1312d90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ **Fun Fact Friday: Miniâ€‘Brains That Can â€œSeeâ€ and Learn!** ðŸ§ ðŸ’¡  \n",
      "\n",
      "Scientists have just taken a giant leap in brainâ€‘onâ€‘aâ€‘chip research: tiny cerebral organoidsâ€”often called â€œminiâ€‘brainsâ€â€”have been taught to respond to light. ðŸŒŸ This is the **first evidence that organoid tissue can exhibit a primitive form of learning**, opening the door to the emerging idea of **â€œorganoid intelligence.â€**  \n",
      "\n",
      "ðŸ”¬ **Why it matters**  \n",
      "1ï¸âƒ£ **A new model for brain research** â€“ Unlike animal models, organoids are humanâ€‘derived, offering a more accurate window into neurodevelopment and disease.  \n",
      "2ï¸âƒ£ **Learning in a dish** â€“ Demonstrating that these structures can adapt to stimuli suggests we can study the fundamentals of learning, memory, and plasticity in a controlled, ethical platform.  \n",
      "3ï¸âƒ£ **Future tech frontier** â€“ Imagine bioâ€‘hybrid systems where living neural tissue interfaces with electronics, leading to novel computing architectures or personalized neuroâ€‘therapies.  \n",
      "\n",
      "ðŸ’­ **What could this mean for us?**  \n",
      "- Faster drug discovery for neurological disorders.  \n",
      "- Ethical, patientâ€‘specific testing grounds for geneâ€‘editing therapies.  \n",
      "- A whole new class of â€œlivingâ€ AI that learns like a brain, not just a silicon circuit.  \n",
      "\n",
      "ðŸ”— **Letâ€™s spark a conversation:**  \n",
      "What opportunitiesâ€”or challengesâ€”do you see emerging from the convergence of organoid biology and artificial intelligence? How might this reshape biotech, neuroscience, or even the broader tech landscape?\n",
      "\n",
      "ðŸ‘‡ Drop your thoughts below! ðŸ‘‡  \n",
      "\n",
      "#Neuroscience #Organoids #BioTech #ArtificialIntelligence #Innovation #FutureOfScience #LearningMachines #ScienceNews #Research #BrainOnAChip\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2d1e7",
   "metadata": {},
   "source": [
    "CONDITIONAL CHAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0304f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"mistralai/mistral-7b-instruct\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class llm_schema(BaseModel):\n",
    "    movie_review_flag: Literal[\"positive\", \"negative\"] = Field(description=\"Flag for the movie review\")\n",
    "    \n",
    "new_llm = llm.with_structured_output(llm_schema)\n",
    "\n",
    "prompt_main = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a movie review analyzer.\"),\n",
    "    (\"user\", \"Analyze the following movie review: {movie_review}, and answer either positive or negative\")\n",
    "])\n",
    "\n",
    "chain = prompt_main | new_llm | dictionary_maker_runnable\n",
    "\n",
    "response = chain.invoke({\"movie_review\": \"The movie was bad\"})\n",
    "print(response)\n",
    "response[\"movie_review_flag\"]\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def dictionary_maker(flag) -> dict:\n",
    "    return flag.model_dump()[\"movie_review_flag\"]\n",
    "\n",
    "dictionary_maker_runnable = RunnableLambda(dictionary_maker)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "string_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "# Linked IN Post Generater\n",
    "\n",
    "prompt_linked_in = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Linked IN Post Generater.\"),\n",
    "    (\"user\", \"Generate a Linked IN Post for the following movie content: {movie_review_flag}\")\n",
    "])\n",
    "\n",
    "chain_linked_in = prompt_linked_in | llm | string_parser\n",
    "\n",
    "# Instagram Post Generator\n",
    "\n",
    "prompt_insta = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a Instagram Post Generater.\"),\n",
    "    (\"user\", \"Generate a Instagram Post for the following movie content: {movie_review_flag}\")\n",
    "])\n",
    "\n",
    "chain_insta = prompt_insta | llm | string_parser\n",
    "\n",
    "\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "conditional_chain = RunnableBranch(\n",
    "    (lambda x: \"positive\" in x, chain_linked_in),\n",
    "    chain_insta\n",
    ")\n",
    "\n",
    "final_chain = prompt_main | new_llm | dictionary_maker_runnable | conditional_chain\n",
    "\n",
    "final_chain.invoke({\"movie_review\": \"The movie was good, i liked it\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903632db",
   "metadata": {},
   "source": [
    "IDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "if os.getenv(\"OPENROUTER_API_KEY\") is None:\n",
    "    raise RuntimeError(\"OpenRouter API Key Not Found\")\n",
    "\n",
    "API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=API_KEY,\n",
    "    model=\"mistralai/mistral-7b-instruct\",\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "class MessageAnalysis(BaseModel):\n",
    "    sentiment: Literal[\"positive\", \"neutral\", \"negative\"] = Field(description=\"Sentiment of the message\")\n",
    "    urgency: Literal[\"low\", \"high\"] = Field(description=\"Urgency of the message\")\n",
    "    customer_message: str = Field(description=\"Customer message\")\n",
    "\n",
    "\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Analyze the customer message below.\n",
    "Message:\n",
    "{customer_message}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "new_llm = llm.with_structured_output(MessageAnalysis)\n",
    "\n",
    "def analysis_parser(llm_output) -> dict:\n",
    "    return llm_output.model_dump()\n",
    "\n",
    "analysis_runnable = RunnableLambda(analysis_parser)\n",
    "\n",
    "\n",
    "\n",
    "thank_you_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Write a friendly response for a happy customer.\n",
    "\n",
    "Customer Message:\n",
    "{customer_message}\n",
    "\n",
    "Rules:\n",
    "- Thank the customer\n",
    "- Encourage continued usage\n",
    "- Mention one benefit\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "info_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Write a clear and professional response to the customer query.\n",
    "\n",
    "Customer Message:\n",
    "{customer_message}\n",
    "\n",
    "Rules:\n",
    "- Neutral tone\n",
    "- Clear explanation\n",
    "- No promotional language\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "escalation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Write a response for a serious customer issue.\n",
    "\n",
    "Customer Message:\n",
    "{customer_message}\n",
    "\n",
    "Rules:\n",
    "- Apologize clearly\n",
    "- Acknowledge urgency\n",
    "- Mention escalation to support team\n",
    "- Ask for additional details if needed\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "apology_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Write a polite apology response.\n",
    "\n",
    "Customer Message:\n",
    "{customer_message}\n",
    "\n",
    "Rules:\n",
    "- Apologize\n",
    "- Reassure the customer\n",
    "- Do not mention escalation\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "thank_you_chain = thank_you_prompt | llm | str_parser\n",
    "info_chain = info_prompt | llm | str_parser\n",
    "escalation_chain = escalation_prompt | llm | str_parser\n",
    "apology_chain = apology_prompt | llm | str_parser\n",
    "\n",
    "conditional_chain = RunnableBranch(\n",
    "    (\n",
    "        lambda x: x[\"sentiment\"] == \"positive\",\n",
    "        thank_you_chain\n",
    "    ),\n",
    "    (\n",
    "        lambda x: x[\"sentiment\"] == \"neutral\",\n",
    "        info_chain\n",
    "    ),\n",
    "    (\n",
    "        lambda x: x[\"sentiment\"] == \"negative\" and x[\"urgency\"] == \"high\",\n",
    "        escalation_chain\n",
    "    ),\n",
    "    apology_chain\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "final_chain = (\n",
    "    analysis_prompt\n",
    "    | new_llm\n",
    "    | analysis_runnable\n",
    "    | conditional_chain\n",
    ")\n",
    "\n",
    "final_chain.invoke({\"customer_message\": \"My payment failed and my account is locked. This needs to be fixed immediately.\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e3dad",
   "metadata": {},
   "source": [
    "AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "print(wikipedia_tool.invoke(\"Langchain\"))\n",
    "\n",
    "# Web Search - DuckDuck Go\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "print(search_tool.invoke(\"What is todays news for stocks about AMD?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72288d78",
   "metadata": {},
   "source": [
    "IDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68579adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    base_url = \"https://openrouter.ai/api/v1\", \n",
    "    model =\"openai/gpt-oss-120b:free\", \n",
    "    temperature = 0.7, \n",
    "    api_key = os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    max_tokens=1000,\n",
    "    timeout=30\n",
    "    # ... (other params)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Tool 1 - News Search Tool\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun(description=\"This is a tool to search the web news\")\n",
    "\n",
    "\n",
    "# Tool 2 - Wikipedia Search Tool\n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(description=\"This is a tool to search Wikipedia\"))\n",
    "\n",
    "# Creating ToolKit\n",
    "\n",
    "ToolKit = [search_tool, wikipedia_tool]\n",
    "\n",
    "ToolKit\n",
    "\n",
    "agent = create_agent(model, tools=ToolKit)\n",
    "agent\n",
    "\n",
    "example_query = \"Give me the latest news about stock market\"\n",
    "\n",
    "events = agent.stream(\n",
    "    {\"messages\": [(\"user\", example_query)]},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
