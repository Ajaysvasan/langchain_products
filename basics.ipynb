{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b39ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1eb938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "# First API call with reasoning\n",
    "response = client.chat.completions.create(\n",
    "  model=\"openai/gpt-oss-120b:free\",\n",
    "  messages=[\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How many r's are in the word 'strawberry'?\"\n",
    "          }\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cbead44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The word **“strawberry”** contains **3** occurrences of the letter **r**.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbf98cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, I need to explain the transformer architecture and how to implement it in PyTorch. Let me start by recalling what I know about transformers. They were introduced in the 2017 paper \"Attention Is All You Need.\" The key components are self-attention mechanisms and positional encoding, right? Unlike RNNs, transformers process sequences in parallel, which makes them faster for certain tasks.\n",
      "\n",
      "First, I should outline the main parts of the transformer model. The encoder and decoder are the two main components. The encoder processes the input sequence, and the decoder generates the output sequence. Each encoder consists of a self-attention layer and a feed-forward network. The decoder has similar layers but also includes an encoder-decoder attention layer to focus on relevant parts of the input.\n",
      "\n",
      "Self-attention is crucial here. It allows each position in the sequence to attend to all positions in the previous layer. The process involves computing queries, keys, and values. The attention scores are calculated using dot products, scaled by the square root of the key dimension, then softmax is applied. After that, the values are multiplied by these scores to get the output.\n",
      "\n",
      "Positional encodings are added to the input embeddings to provide information about the position of each token in the sequence. Since transformers don't have inherent order, these encodings help the model understand the sequence structure. They can be learned or fixed using sine and cosine functions as in the original paper.\n",
      "\n",
      "Now, for the PyTorch implementation. I need to build the encoder and decoder layers. PyTorch has a `nn.Transformer` module, but maybe the user wants to build it from scratch? Or perhaps they want to use the built-in layers. I should mention both approaches.\n",
      "\n",
      "Starting with building from scratch. The encoder would consist of multiple encoder layers. Each encoder layer has a multi-head self-attention mechanism followed by a feed-forward network, with residual connections and layer normalization around each.\n",
      "\n",
      "The decoder is similar but has an additional encoder-decoder attention layer. Also, the self-attention in the decoder is masked to prevent attending to future tokens.\n",
      "\n",
      "Implementing multi-head attention: Split the queries, keys, and values into multiple heads, apply attention in parallel, then concatenate the results. The `nn.MultiheadAttention` class in PyTorch can be used here.\n",
      "\n",
      "Positional encodings: Implementing the sine and cosine functions for fixed positional encodings. For a sequence of length L and embedding dimension D, each position's encoding is a vector where even indices are sine and odd are cosine functions of different wavelengths.\n",
      "\n",
      "Putting it all together. The transformer model would have an embedding layer for the input tokens, add positional encodings, pass through multiple encoder layers, then the decoder with its layers, and finally a linear layer to produce output logits.\n",
      "\n",
      "In PyTorch, using `nn.Transformer` might be easier. The user can specify the number of layers, heads, etc. But understanding the components is important for customization.\n",
      "\n",
      "Potential issues to mention: Handling variable-length sequences, padding masks, and sequence masks for the decoder. Also, training with teacher forcing for the decoder.\n",
      "\n",
      "I should outline the steps for building the model in PyTorch, perhaps with code examples. Show how to define the encoder and decoder, use multi-head attention, apply positional encodings, and then train the model with an example like translation.\n",
      "\n",
      "Wait, maybe the user wants a more hands-on example. Let me think of a simple example, maybe a toy model for sequence-to-sequence tasks. Also, mention the use of `nn.Embedding` for token embeddings, combining with positional encodings, and the overall structure.\n",
      "\n",
      "Testing the code is important. Maybe include a small example with dummy data to show how the model works. Also, note that training would require a loss function like cross-entropy and an optimizer like Adam with learning rate scheduling as in the paper.\n",
      "\n",
      "I should also mention some key hyperparameters: number of heads, number of layers, embedding dimension, hidden dimension in feed-forward networks, dropout rates, etc.\n",
      "\n",
      "Let me structure the answer step by step. First, explain the transformer architecture, then move to PyTorch implementation, starting with components like positional encoding, then encoder and decoder layers, and then the full model. Provide code examples for each part, and finally a complete example using PyTorch's built-in modules.\n",
      "\n",
      "Make sure to clarify that while the built-in `nn.Transformer` is convenient, understanding the components helps in customizing the model. Also, note that the original transformer is for machine translation, but the architecture is used in many NLP tasks now.\n",
      "\n",
      "Check if I'm missing any important parts. Oh, residual connections and layer normalization are essential in each sublayer (attention and feed-forward). Also, the paper uses dropout in various places. Need to include that in the code examples.\n",
      "\n",
      "Okay, putting it all together now.\n",
      "</think>\n",
      "\n",
      "The **Transformer architecture**, introduced in the 2017 paper *\"Attention Is All You Need\"*, is a neural network design that relies entirely on **self-attention mechanisms** to process sequential data in parallel. It has become foundational for NLP tasks like translation, text generation, and more. Below is a breakdown of its components and how to implement it in **PyTorch**.\n",
      "\n",
      "---\n",
      "\n",
      "### **Transformer Architecture Overview**\n",
      "\n",
      "#### **Key Components**\n",
      "1. **Self-Attention Mechanism**:\n",
      "   - Computes relationships between elements in a sequence.\n",
      "   - Uses **queries (Q)**, **keys (K)**, and **values (V)** derived from input embeddings.\n",
      "   - Attention scores:  \n",
      "     $$\n",
      "     \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
      "     $$\n",
      "     where $d_k$ is the dimension of keys (scaling prevents large dot products).\n",
      "\n",
      "2. **Multi-Head Attention**:\n",
      "   - Splits Q, K, V into multiple \"heads\" to capture different aspects of relationships.\n",
      "   - Concatenates outputs from all heads and applies a linear projection.\n",
      "\n",
      "3. **Positional Encodings**:\n",
      "   - Adds position-specific information to embeddings since Transformers lack inherent sequence order.\n",
      "   - Uses sine/cosine functions for fixed encodings:\n",
      "     $$\n",
      "     PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right), \\quad PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
      "     $$\n",
      "     where $d$ is the embedding dimension.\n",
      "\n",
      "4. **Feed-Forward Networks (FFN)**:\n",
      "   - Two linear layers with a ReLU activation applied after attention.\n",
      "\n",
      "5. **Encoder-Decoder Structure**:\n",
      "   - **Encoder**: Processes input (e.g., source text) with self-attention and FFN.\n",
      "   - **Decoder**: Generates output (e.g., target text) using masked self-attention, encoder-decoder attention, and FFN.\n",
      "\n",
      "---\n",
      "\n",
      "### **Implementing Transformers in PyTorch**\n",
      "\n",
      "#### **1. Positional Encoding**\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import math\n",
      "\n",
      "class PositionalEncoding(nn.Module):\n",
      "    def __init__(self, d_model, max_len=5000):\n",
      "        super().__init__()\n",
      "        pe = torch.zeros(max_len, d_model)\n",
      "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
      "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
      "        pe[:, 0::2] = torch.sin(position * div_term)\n",
      "        pe[:, 1::2] = torch.cos(position * div_term)\n",
      "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
      "        self.register_buffer('pe', pe)\n",
      "\n",
      "    def forward(self, x):\n",
      "        return x + self.pe[:, :x.size(1)]\n",
      "```\n",
      "\n",
      "#### **2. Encoder Layer**\n",
      "```python\n",
      "class TransformerEncoderLayer(nn.Module):\n",
      "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
      "        super().__init__()\n",
      "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
      "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
      "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
      "        self.norm1 = nn.LayerNorm(d_model)\n",
      "        self.norm2 = nn.LayerNorm(d_model)\n",
      "        self.dropout = nn.Dropout(dropout)\n",
      "\n",
      "    def forward(self, src):\n",
      "        # Self-attention\n",
      "        src2, _ = self.self_attn(src, src, src)\n",
      "        src = src + self.dropout(src2)\n",
      "        src = self.norm1(src)\n",
      "        \n",
      "        # Feed-forward network\n",
      "        src2 = self.linear2(torch.relu(self.linear1(src)))\n",
      "        src = src + self.dropout(src2)\n",
      "        src = self.norm2(src)\n",
      "        return src\n",
      "```\n",
      "\n",
      "#### **3. Full Transformer Model**\n",
      "```python\n",
      "class TransformerModel(nn.Module):\n",
      "    def __init__(self, ntoken, d_model, nhead, num_encoder_layers, dim_feedforward=2048, max_len=5000):\n",
      "        super().__init__()\n",
      "        self.encoder = nn.Embedding(ntoken, d_model)\n",
      "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
      "        encoder_layers = [TransformerEncoderLayer(d_model, nhead, dim_feedforward) for _ in range(num_encoder_layers)]\n",
      "        self.encoders = nn.ModuleList(encoder_layers)\n",
      "        self.fc = nn.Linear(d_model, ntoken)  # Final output layer\n",
      "\n",
      "    def forward(self, src):\n",
      "        src = self.encoder(src) * math.sqrt(self.d_model)  # Scale embeddings\n",
      "        src = self.pos_encoder(src)\n",
      "        for enc in self.encoders:\n",
      "            src = enc(src)\n",
      "        output = self.fc(src)\n",
      "        return output\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "#### **4. Using PyTorch’s Built-in `nn.Transformer`**\n",
      "PyTorch provides a high-level `nn.Transformer` class for quick implementation:\n",
      "```python\n",
      "import torch.nn as nn\n",
      "\n",
      "model = nn.Transformer(\n",
      "    d_model=512, \n",
      "    nhead=8, \n",
      "    num_encoder_layers=6, \n",
      "    num_decoder_layers=6\n",
      ")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Training Example (Dummy Data)**\n",
      "```python\n",
      "# Dummy data: batch_size=2, seq_len=10, vocab_size=1000\n",
      "src = torch.randint(0, 1000, (10, 2))  # (sequence_length, batch_size)\n",
      "tgt = torch.randint(0, 1000, (10, 2))\n",
      "\n",
      "# Initialize model\n",
      "model = TransformerModel(ntoken=1000, d_model=512, nhead=8, num_encoder_layers=3)\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
      "\n",
      "# Forward pass\n",
      "output = model(src)\n",
      "loss = criterion(output.view(-1, 1000), tgt.view(-1))\n",
      "loss.backward()\n",
      "optimizer.step()\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Considerations**\n",
      "- **Masking**: Use `nn.Transformer.generate_square_subsequent_mask` for decoder input to prevent future token leakage.\n",
      "- **Padding Masks**: Exclude padding tokens during attention computation.\n",
      "- **Optimization**: Use the Adam optimizer with a learning rate scheduler as described in the original paper.\n",
      "\n",
      "---\n",
      "\n",
      "### **Applications**\n",
      "Transformers are used in:\n",
      "- **Machine Translation** (e.g., Google's BERT, T5)\n",
      "- **Text Generation** (e.g., GPT series)\n",
      "- **Code Generation** (e.g., Codex)\n",
      "- **Multimodal Tasks** (e.g., Vision Transformers)\n",
      "\n",
      "By understanding the core components and PyTorch implementation, you can customize transformers for various tasks!"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me about transformer architecture and how to do it in pytorch\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=0.6,\n",
    "    max_completion_tokens=4096,\n",
    "    top_p=0.95,\n",
    "    reasoning_effort=\"default\",\n",
    "    stream=True,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
